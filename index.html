<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Introduction to Apache Spark</title>

		<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
		<meta name="author" content="Hakim El Hattab">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/font-awesome.min.css">
		<link rel="stylesheet" href="css/theme/white.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<img src="img/spark-logo.png" style="width: 600px; border: none; box-shadow: none;">
					<h2>Analyzing Wikipedia Event Streams using Spark 2.0 and GraphX</h2>
					<p>
						<small><a href="https://www.linkedin.com/in/mylesdanielbaker">Myles Baker </a>/ <a href="http://www.captechconsulting.com/careers/why-captech">CapTech</a> / <a href="http://twitter.com/mydpy">@mydpy</a></small>
					</p>
				</section>

				<section>
					<img src="img/captech.png" style="width: 800px; border: none; box-shadow: none; PADDING-LEFT: 5px; PADDING-RIGHT: 5px"><br>
                    <img src="img/best-firms.png" style="width: 100px; border: none; box-shadow: none; PADDING-LEFT:50px; PADDING-RIGHT: 50px"><img src="img/inc5000.png" style="width: 100px; border: none; box-shadow: none; PADDING-LEFT:50px; PADDING-RIGHT: 50px"><img src="img/vault.png" style="width: 100px; border: none; box-shadow: none; PADDING-LEFT:50px; PADDING-RIGHT: 50px">
				</section>
                
                

				<section>
					<section>
						<h2>What is Spark?</h2>
					</section>

					<section>
                        <h3><a href="https://spark.apache.org">Apache Spark</a> is a cluster computing platform designed to be <i>fast</i> and <i>general-purpose</i></h3>
                        
                        <ul>
						<li>Optimized for <i>in-memory</i> computing on large datasets</li>
                        <li>General execution model supports batch and continuous applications</li>
                        <li>Native APIs in Java, Scala, Python, and R</li>
                        <li>Easy-to-use API and application integration</li>
                        <li>Fast uptime for data engineering applications</li> 
                        <li>Empowers analysts and stakeholders</li>
                        </ul>
					</section>
                    <section data-background-video="video/spark-stack-web.mov" 
                            data-background-color="#000">
                    </section>
				</section>
				<section>
                    <section>
                        <h2>Why Spark?</h2>
                    </section>
					<section>
						<img src="img/hadoop-logo.jpeg" style="width: 600px; border: none; box-shadow: none;">
						<img src="img/map-reduce.png" style="width: 900px; border: none; box-shadow: none;">
					</section>
					<section>
						<h3>This paradigm unlocks intelligence on your data not previously possible</h3>
                        <h4>...but is designed for batch applications and disk I/O. What about real-time?</h4>
					</section>
					<section>
						<h3>...that requires special tools and imposes resource demands</h3>
                        <div style="width:100%;">
						<img src="img/spark-vs-hadoop.png" style="width: 800px; border: none; box-shadow: none;">
                    </div>
                    </section>
                    <section>
                        <h3>Spark is generating a lot of interest</h3>
                        <iframe scrolling="no" style="border:none;" width="600" height="400" src="http://www.google.com/trends/fetchComponent?hl=en-US&q=apache+spark,+apache+hadoop&date=1/2010+75m&cmpt=q&tz&tz&content=1&cid=TIMESERIES_GRAPH_AVERAGES_CHART&export=5&w=600&h=400"></iframe>
                    </section>
                    
    				<section>
    					<h3>The Open Source Community is Abandoning Hadoop MapReduce</h3>
    					<br />
    					<div style="width:100%;">
    					<div style="float:left; width:50%;">
    					<p>MapReduce</p>
    					<img width="400" src="img/asf-hadoop-mr.png", style="border:1px;"> 
    					</div>
    					<div style="float:right; width:50%:">
    					<p>Spark</p>
    					<img width="400" src="img/asf-spark.png", style="border:1px;">
    					</div>
    					</div>					
    				</section>
                    
    				<section>
                        <h3>...And So is Cloudera</h3>
    					<div style="width:100%;">
    					<img width="1600" src="img/cloudera-1.png", style="border:1px;"> 
    					</div>					
    				</section>

    				<section>
    					<div style="width:100%;">
    					<img width="1600" src="img/cloudera-2.png", style="border:1px;"> 
    					</div>					
    				</section>
                    
    				<section>
    					<div style="width:100%;">
    					<img width="1600" src="img/cloudera-3.png", style="border:1px;"> 
    					</div>					
    				</section>
                    
    				<section>
                        <h3>...Oh, and IBM Invested 300 Million Too</h3>
    					<div style="width:100%;">
    					<img width="1600" src="img/IBM-1.png", style="border:1px;"> 
    					</div>					
    				</section>
                    
                    <section>
                        <h3>How fast is Spark?</h3>
                        <ul>
                            <li>3x as fast</li>
                            <li>10x fewer machines</li>
                        </ul>
                        <img src="img/spark-100tb.png" style="width: 800px; border: none; box-shadow: none;">
                    </section>

                    

				</section>

                <section>
                    <section>
                        <h2>How does Spark work?</h2>
                    </section>
					<section>
                        <h3>A new abstraction: <a href="https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf">Resilient, Distributed Datasets</a></h3>
                        <img src="img/rdds-explained.png" style="width: 800px; border: none; box-shadow: none;">
                        <p style="font-size:50%"><b>Resilient Distributed Dataset:</b> An abstraction that enables efficient data reuse in distributed processing environments</p>
					</section>
					<section>
                        <h2>RDD focal points:</h2>
						<ul>
                            <li>Fault-tolerant, parallel data structure</li>
                            <li>For interactive, iterative operations</li>
                            <li>Immutable</li>
                            <li>Transformations are logged and performed <i>lazily</i></li>
                            <li>User-controlled persistence and partitioning</li>
                        </ul>
					</section>
					<section>
                        <h2>Spark's implementation of RDDs:</h2>
                        <ul>
                            <li>Language-intergrated interface (functions inline, etc.)</li>
                            <li>Runs inside a JVM to easily interact with Hadoop filesystem</li>
                            <li>Statically typed for performance</li>
                        </ul>
					</section>
					<section>
                        <h2>What's New in Spark 2.0?</h2>
                        <ul>
                            <li>Tungsten Enhancements
                                <ul>
                                    <li>Enhancements to internal memory management</li>
                                    <li><a href="https://issues.apache.org/jira/browse/SPARK-12785">SPARK-12785</a></li>
                                    <li><a href="https://issues.apache.org/jira/browse/SPARK-8641">SPARK-8641</a></li>
                                </ul></li>
                            <li>Structured Streaming
                                <ul>
                                    <li>Streaming DataFrame for Continuous Applications</li>
                                    <li><a href="https://issues.apache.org/jira/browse/SPARK-8360">SPARK-8360</a></li>
                                </ul></li>
                            <li>Whole-stage Code Generation</li>
                            <li>Optimized input/output using Parquet (columnar data storage format)</li>
                            <li>Translations between Dataframes (Spark 1.3) and Datasets (Spark 1.6)</li>
                        </ul>
					</section>
                </section>

				<!--SCALA
                <section>
                    <section>
                        <h2>Demo</h2>
                    </section>
                    <section>
                        <h3>Questions before we get started?</h3>
                    </section>
                    
                    <section>
                        <h3>Getting started with Spark</h3>
                        <ul style="font-size:80%">
                            <li>Download the <a href="https://github.com/mydpy/rva-data-hackers-spark-demo">RVA Data Hackers Github Repository</a>.</li>
                            <li>Follow the instructions in README.md</li>
                        </ul>
                    </section>
                    
                    <section>
                        <h4>Launching Spark</h4>
                        <pre><code data-trim contenteditable>
>spark-shell
                        </code></pre>
                        <pre><code data-trim contenteditable>
>pyspark
                        </code></pre>
                        <pre><code data-trim contenteditable>
>spark-submit --class &#60;class_name> --master &#60;context_mode> \
>&#60;jar_file> [args]
                        </code></pre>
                    </section>
                
                    <section>
                        <h3>Interactive Spark</h3>
                        </code></pre>
                        <pre><code data-trim contenteditable>
15/04/16 11:50:48 INFO SecurityManager: Changing view acls to: cloudera
15/04/16 11:50:48 INFO SecurityManager: Changing modify acls to: cloudera
15/04/16 11:50:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
15/04/16 11:50:48 INFO HttpServer: Starting HTTP Server
15/04/16 11:50:48 INFO Utils: Successfully started service 'HTTP class server' on port 43049.
Welcome to
   ____              __
  / __/__  ___ _____/ /__
 _\ \/ _ \/ _ `/ __/  '_/
/___/ .__/\_,_/_/ /_/\_\   version 1.2.0
   /_/

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_67)
Type in expressions to have them evaluated.
Type :help for more information.
15/04/16 11:50:54 WARN Utils: Your hostname, quickstart.cloudera resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface eth0)
15/04/16 11:50:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
15/04/16 11:50:54 INFO SecurityManager: Changing view acls to: cloudera
15/04/16 11:50:54 INFO SecurityManager: Changing modify acls to: cloudera
15/04/16 11:50:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
15/04/16 11:50:55 INFO Slf4jLogger: Slf4jLogger started
15/04/16 11:50:55 INFO Remoting: Starting remoting
15/04/16 11:50:55 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.0.2.15:51331]
15/04/16 11:50:55 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@10.0.2.15:51331]
15/04/16 11:50:55 INFO Utils: Successfully started service 'sparkDriver' on port 51331.
15/04/16 11:50:56 INFO SparkEnv: Registering MapOutputTracker
15/04/16 11:50:56 INFO SparkEnv: Registering BlockManagerMaster
15/04/16 11:50:56 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20150416115056-a439
15/04/16 11:50:56 INFO MemoryStore: MemoryStore started with capacity 265.4 MB
15/04/16 11:50:56 INFO HttpFileServer: HTTP File server directory is /tmp/spark-cc9f46f6-5288-48b0-b106-ee9281b10156
15/04/16 11:50:56 INFO HttpServer: Starting HTTP Server
15/04/16 11:50:56 INFO Utils: Successfully started service 'HTTP file server' on port 41679.
15/04/16 11:50:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/04/16 11:50:56 INFO SparkUI: Started SparkUI at http://10.0.2.15:4040
15/04/16 11:50:57 INFO AppClient$ClientActor: Connecting to master spark://quickstart.cloudera:7077...
15/04/16 11:50:57 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20150416115057-0001
15/04/16 11:50:57 INFO AppClient$ClientActor: Executor added: app-20150416115057-0001/0 on worker-20150408095110-10.0.2.15-7078 (10.0.2.15:7078) with 2 cores
15/04/16 11:50:57 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150416115057-0001/0 on hostPort 10.0.2.15:7078 with 2 cores, 512.0 MB RAM
15/04/16 11:50:57 INFO AppClient$ClientActor: Executor updated: app-20150416115057-0001/0 is now LOADING
15/04/16 11:50:57 INFO AppClient$ClientActor: Executor updated: app-20150416115057-0001/0 is now RUNNING
15/04/16 11:50:58 INFO NettyBlockTransferService: Server created on 40182
15/04/16 11:50:58 INFO BlockManagerMaster: Trying to register BlockManager
15/04/16 11:50:58 INFO BlockManagerMasterActor: Registering block manager 10.0.2.15:40182 with 265.4 MB RAM, BlockManagerId(<driver>, 10.0.2.15, 40182)
15/04/16 11:50:58 INFO BlockManagerMaster: Registered BlockManager
15/04/16 11:51:07 INFO EventLoggingListener: Logging events to hdfs://quickstart.cloudera:8020/user/spark/applicationHistory/app-20150416115057-0001
15/04/16 11:51:13 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
15/04/16 11:51:14 INFO SparkILoop: Created spark context..
Spark context available as sc.

scala> 15/04/16 11:51:18 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@10.0.2.15:47007/user/Executor#-876449197] with ID 0
15/04/16 11:51:20 INFO BlockManagerMasterActor: Registering block manager 10.0.2.15:52204 with 265.4 MB RAM, BlockManagerId(0, 10.0.2.15, 52204)
                        </code></pre>

                        <pre><code data-trim contenteditable>
scala> sc
res38: org.apache.spark.SparkContext = org.apache.spark.SparkContext@5f4d088e
                        </code></pre>
                        <pre><code data-trim contenteditable>
In [1]: sc
Out[1]: <pyspark.context.SparkContext at 0x10cae8e10>
                        </code></pre>
                    </section>
                    <section>
                        <h3>Creating an RDD</h3>
                        <pre><code data-trim contenteditable>
scala> val rdd = sc.textFile("intro/data/boa-constrictor")
                        </code></pre>
                        <pre><code data-trim contenteditable>
15/04/16 12:36:03 INFO MemoryStore: ensureFreeSpace(259918) called with curMem=2248344, maxMem=278302556
15/04/16 12:36:03 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 253.8 KB, free 263.0 MB)
15/04/16 12:36:04 INFO MemoryStore: ensureFreeSpace(21134) called with curMem=2508262, maxMem=278302556
15/04/16 12:36:04 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 20.6 KB, free 263.0 MB)
15/04/16 12:36:04 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.2.15:40182 (size: 20.6 KB, free: 265.2 MB)
15/04/16 12:36:04 INFO BlockManagerMaster: Updated info of block broadcast_32_piece0
15/04/16 12:36:04 INFO SparkContext: Created broadcast 32 from textFile at &#60;console>:12
rdd: org.apache.spark.rdd.RDD[String] = spark-input/boa-constrictor MappedRDD[40] at textFile at &#60;console>:12

                        </code></pre>

                        <pre><code data-trim contenteditable>
scala> val silverstein = rdd.collect
...
silverstein: Array[String] = Array(
    Im being swallered by a Boa Constrictor, a Boa Constrictor, a Boa Constrictor,
    Im being swallered by a Boa Constrictor,
    and I dont - like snakes - one bit!,
    Oh no, he swallered my toe.,
    Oh gee, he swallered my knee.,
    Oh fiddle, he swallered my middle.,
    Oh what a pest, he swallered my chest.,
    Oh heck, he swallered my neck.,
    Oh, dread, he swallered my - (BURP)
)
                        </code></pre>
                    </section>
                    <section>
                        <h3>Transforming an RDD</h3>
                        <pre><code data-trim contenteditable>
val BoaConstrictor = rdd.filter(line => line.contains("Boa"))
                        </code></pre>
                        <pre><code data-trim contenteditable>
BoaConstrictor.collect
res39: Array[String] = Array(
    Im being swallered by a Boa Constrictor, a Boa Constrictor, a Boa Constrictor,
    Im being swallered by a Boa Constrictor
)
                        </code></pre>
                    </section>

                    <section>
                        <h3>Acting on an RDD</h3>
                        <pre><code data-trim contenteditable>
val words = rdd.flatMap(line => line.split(" "))
val counts = words.map(word => (word,1)).reduceByKey{case (x,y) => x+y}
                        </code></pre>
                        <pre><code data-trim contenteditable>
counts.collect
res40: Array[(String, Int)] = Array(
    (don't,1), (pest,,1), (fiddle,,1), (one,1), (bit!,1), ((BURP),1),
    (toe.,1), (Boa,4), (Constrictor,,1), (my,6), (what,1), (dread,,1),
    (Constrictor,3), (heck,,1), (neck.,1), (swallered,8), (Oh,,1),
    (a,5), (snakes,1), (no,,1), (I,1), (he,6), (Oh,5), (middle.,1),
    (by,2), (-,3), (like,1), (I'm,2), (and,1), (chest.,1), (gee,,1), (being,2), (knee.,1)
)
                        </code></pre>
                    </section>

                    <section>
                        <h3>Analysis using RDDs</h3>
                        <pre><code data-trim contenteditable>
val Boa = counts.filter(pair => pair._1.equals("Boa"))
                        </code></pre>
                        <pre><code data-trim contenteditable>
Boa.collect
res41: Array[(String, Int)] = Array((Boa,4))
                        </code></pre>
                    </section>
                    
					<section>
                        <h3>Spark SQL: An Analyst's Best Friend</h2>
                        <pre><code data-trim contenteditable>
val df = sqlContext.read.text("intro/data/boa-constrictor")
df.registerTempTable("df")
                        </code></pre>
            </section>
            <section>
                        <pre><code data-trim contenteditable>
sqlContext.sql("select * from df where value like '%Boa%'").show()
                        </code></pre>
                        <pre><code data-trim contenteditable>
+--------------------+
|               value|
+--------------------+
|Im being swallere...|
|Im being swallere...|
+--------------------+
                        </code></pre>
                        <pre><code data-trim contenteditable>
sqlContext.sql("select * from df where value like '%Boa%'").take(10)
                        </code></pre>

                        <pre><code data-trim contenteditable>
res24: Array[org.apache.spark.sql.Row] = Array(
    [Im being swallered by a Boa Constrictor, a Boa Constrictor, a Boa Constrictor,], 
    [Im being swallered by a Boa Constrictor,])
                        </code></pre>
                    </section>
                    <section>
                        <pre><code data-trim contenteditable>
val words = df.flatMap(row => row.toString.split(" "))
    .map(word => (word,1))
words.registerTempTable("words")
                        </code></pre>
            </section>
            <section>
                        <pre><code data-trim contenteditable>
sqlContext.sql("""
    SELECT _1, count(*)
        FROM words 
        WHERE _1 = 'Boa'
        GROUP BY _1 
        ORDER BY count(*) DESC
""").show()
                        </code></pre>
                        <pre><code data-trim contenteditable>
+---+--------+
| _1|count(1)|
+---+--------+
|Boa|       4|
+---+--------+
                        </code></pre>
            </section>
            <section>
                        <pre><code data-trim contenteditable>
sqlContext.sql("""
    SELECT _1, sum(_2)
        FROM words 
        WHERE _1 = 'Boa'
        GROUP BY _1 
        ORDER BY count(_2) DESC
""").show()
                        </code></pre>
                        <pre><code data-trim contenteditable>
+---+-------+
| _1|sum(_2)|
+---+-------+
|Boa|      4|
+---+-------+
                        </code></pre>
                    </section>

                    <section>
                    <h3>Java Example using Spark-submit</h3>
                        <pre><code data-trim contenteditable>
import java.util.Arrays;
import java.util.List;
import java.lang.Iterable;
import scala.Tuple2;
import org.apache.commons.lang.StringUtils;
import org.apache.spark.api.java.*
import org.apache.spark.api.java.function.*

public class WordCount {
  public static void main(String[] args) throws Exception {
    String master = "local";
    JavaSparkContext sc = new JavaSparkContext(
      master, "wordcount", System.getenv("SPARK_HOME"), System.getenv("JARS"));
    JavaRDD&#60;String> rdd = sc.textFile("spark-input/boa-constrictor");
    JavaPairRDD&#60;String, Integer> counts = rdd.flatMap(
      new FlatMapFunction&#60;String, String>() {
        public Iterable&#60;String> call(String x) {
          return Arrays.asList(x.split(" "));
        }}).mapToPair(new PairFunction&#60;String, String, Integer>(){
          public Tuple2&#60;String, Integer> call(String x){
            return new Tuple2(x, 1);
          }}).reduceByKey(new Function2&#60;Integer, Integer, Integer>(){
              public Integer call(Integer x, Integer y){ return x+y;}});
  counts.saveAsTextFile("output/boa-constrictor");
  }
}
                        </code></pre>
                        <pre><code data-trim contenteditable>
>spark-submit --class WordCount --master local \
>spark-demo.jar
                        </code></pre>
                    </section>
				</section>-->

				<!--PYTHON-->
                <section>
                    <section>
                        <h2>Demo</h2>
                    </section>
                    <section>
                        <h3>Questions before we get started?</h3>
                    </section>
                    
                    <section>
                        <h3>Getting started with Spark</h3>
                        <ul style="font-size:80%">
                            <li>Download the <a href="https://github.com/mydpy/rva-data-hackers-spark-demo">RVA Data Hackers Github Repository</a>.</li>
                            <li>Follow the instructions in README.md</li>
                        </ul>
                    </section>
                    
                    <section>
                        <h4>Launching Spark</h4>
                        <pre><code data-trim contenteditable>
>spark-shell
                        </code></pre>
                        <pre><code data-trim contenteditable>
>pyspark
                        </code></pre>
                        <pre><code data-trim contenteditable>
>spark-submit --class &#60;class_name> --master &#60;context_mode> \
>&#60;jar_file> [args]
                        </code></pre>
                    </section>
                
                    <section>
                        <h3>Interactive Spark</h3>
                        </code></pre>
                        <pre><code data-trim contenteditable>
16/03/15 09:20:05 INFO Utils: Successfully started service 'SparkUI' on port 4043.
16/03/15 09:20:05 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.23.80:4043
16/03/15 09:20:05 INFO Executor: Starting executor ID driver on host localhost
16/03/15 09:20:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57673.
16/03/15 09:20:05 INFO NettyBlockTransferService: Server created on 57673
16/03/15 09:20:05 INFO BlockManagerMaster: Trying to register BlockManager
16/03/15 09:20:05 INFO BlockManagerMasterEndpoint: Registering block manager localhost:57673 with 511.1 MB RAM, BlockManagerId(driver, localhost, 57673)
16/03/15 09:20:05 INFO BlockManagerMaster: Registered BlockManager
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.0.0-SNAPSHOT
      /_/

Using Python version 2.7.10 (default, Jul 14 2015 19:46:27)
SparkContext available as sc, SQLContext available as sqlContext.
                        </code></pre>

                        <pre><code  class="python" data-trim contenteditable>
In [1]: sc
Out[1]: <pyspark.context.SparkContext at 0x10cae8e10>
                        </code></pre>
                    </section>
                    <section>
                        <h3>Creating an RDD</h3>
                        <pre><code  class="python" data-trim contenteditable>
rdd = sc.textFile("intro/data/boa-constrictor")
                        </code></pre>
                        <pre><code  class="python" data-trim contenteditable>
16/03/15 09:21:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 107.7 KB)
16/03/15 09:21:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 117.8 KB)
16/03/15 09:21:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:57673 (size: 10.2 KB, free: 511.1 MB)
16/03/15 09:21:49 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2
                        </code></pre>

                        <pre><code  class="python" data-trim contenteditable>
silverstein = rdd.collect()
...
[u'Im being swallered by a Boa Constrictor, a Boa Constrictor, a Boa Constrictor,',
 u'Im being swallered by a Boa Constrictor,',
 u'and I dont - like snakes - one bit!,',
 u'Oh no, he swallered my toe.,',
 u'Oh gee, he swallered my knee.,',
 u'Oh fiddle, he swallered my middle.,',
 u'Oh what a pest, he swallered my chest.,',
 u'Oh heck, he swallered my neck.,',
 u'Oh, dread, he swallered my - (BURP)']
                        </code></pre>
                    </section>
                    <section>
                        <h3>Transforming an RDD</h3>
                        <pre><code  class="python" data-trim contenteditable>
BoaConstrictor = rdd.filter(lambda line: "Boa" in line)
                        </code></pre>
                        <pre><code  class="python" data-trim contenteditable>
BoaConstrictor.collect()
Out[16]: 
[u'Im being swallered by a Boa Constrictor, a Boa Constrictor, a Boa Constrictor,',
 u'Im being swallered by a Boa Constrictor,']
                        </code></pre>
                    </section>

                    <section>
                        <h3>Acting on an RDD</h3>
                        <pre><code  class="python" data-trim contenteditable>
words = rdd.flatMap(lambda line: line.split(" "))
counts = words.map(lambda word:(word,1)).reduceByKey(lambda x,y: x+y)
                        </code></pre>
                        <pre><code  class="python" data-trim contenteditable>
counts.collect()
Out[23]: 
[(u'a', 5),
 (u'and', 1),
 (u'what', 1),
 (u'(BURP)', 1),
 (u'being', 2),
 (u'heck,', 1),
 (u'chest.,', 1),
 (u'-', 3),
 (u'Constrictor,', 4),
 (u'I', 1),
 (u'Im', 2),
 (u'swallered', 8),
 (u'Oh,', 1),
 (u'no,', 1),
 (u'dread,', 1),
 (u'my', 6),
 (u'knee.,', 1),
 (u'like', 1),
 (u'snakes', 1),
 (u'toe.,', 1),
 (u'Oh', 5),
 (u'neck.,', 1),
 (u'pest,', 1),
 (u'one', 1),
 (u'fiddle,', 1),
 (u'middle.,', 1),
 (u'bit!,', 1),
 (u'gee,', 1),
 (u'Boa', 4),
 (u'he', 6),
 (u'by', 2),
 (u'dont', 1)]
                        </code></pre>
                    </section>

                    <section>
                        <h3>Analysis using RDDs</h3>
                        <pre><code  class="python" data-trim contenteditable>
Boa = counts.filter(lambda pair: pair[0] == "Boa")
                        </code></pre>
                        <pre><code  class="python" data-trim contenteditable>
Boa.collect()
Out[26]: [(u'Boa', 4)]
                        </code></pre>
                    </section>
                    
					<section>
                        <h3>Spark SQL: An Analyst's Best Friend</h2>
                        <pre><code  class="python" data-trim contenteditable>
df = sqlContext.read.text("intro/data/boa-constrictor")
df.registerTempTable("df")
                        </code></pre>
            </section>
            <section>
                        <pre><code  class="python" data-trim contenteditable>
sqlContext.sql("select * from df where value like '%Boa%'").show()
                        </code></pre>
                        <pre><code data-trim contenteditable>
+--------------------+
|               value|
+--------------------+
|Im being swallere...|
|Im being swallere...|
+--------------------+
                        </code></pre>
                        <pre><code class="python" data-trim contenteditable>
sqlContext.sql("select * from df where value like '%Boa%'").take(10)
                        </code></pre>

                        <pre><code class="python" data-trim contenteditable>
Out[30]: 
[Row(value=u'Im being swallered by a Boa Constrictor, a Boa Constrictor, a Boa Constrictor,'),
 Row(value=u'Im being swallered by a Boa Constrictor,')]
                        </code></pre>
                    </section>
                    <section>
                        <pre><code class="python" data-trim contenteditable>
words = df.rdd.flatMap(
        lambda row: str(row.asDict()['value'])\
        .split(" "))\
        .map(lambda word: (word,1))
words.toDF().registerTempTable("words")
                        </code></pre><br>
                        Note: DataSet not yet supported in PySpark
            </section>
            <section>
                        <pre><code  class="python" data-trim contenteditable>
sqlContext.sql("""
    SELECT _1, count(*)
        FROM words 
        WHERE _1 = 'Boa'
        GROUP BY _1 
        ORDER BY count(*) DESC
""").show()
                        </code></pre>
                        <pre><code  class="python" data-trim contenteditable>
+---+--------+
| _1|count(1)|
+---+--------+
|Boa|       4|
+---+--------+
                        </code></pre>
            </section>
            <section>
                        <pre><code  class="python" data-trim contenteditable>
sqlContext.sql("""
    SELECT _1, sum(_2)
        FROM words 
        WHERE _1 = 'Boa'
        GROUP BY _1 
        ORDER BY count(_2) DESC
""").show()
                        </code></pre>
                        <pre><code  class="python" data-trim contenteditable>
+---+-------+
| _1|sum(_2)|
+---+-------+
|Boa|      4|
+---+-------+
                        </code></pre>
                    </section>

                    <section>
                    <h3>Java Example using Spark-submit</h3>
                        <pre><code  class="java" data-trim contenteditable>
import java.util.Arrays;
import java.util.List;
import java.lang.Iterable;
import scala.Tuple2;
import org.apache.commons.lang.StringUtils;
import org.apache.spark.api.java.*
import org.apache.spark.api.java.function.*

public class WordCount {
  public static void main(String[] args) throws Exception {
    String master = "local";
    JavaSparkContext sc = new JavaSparkContext(
      master, "wordcount", System.getenv("SPARK_HOME"), System.getenv("JARS"));
    JavaRDD&#60;String> rdd = sc.textFile("spark-input/boa-constrictor");
    JavaPairRDD&#60;String, Integer> counts = rdd.flatMap(
      new FlatMapFunction&#60;String, String>() {
        public Iterable&#60;String> call(String x) {
          return Arrays.asList(x.split(" "));
        }}).mapToPair(new PairFunction&#60;String, String, Integer>(){
          public Tuple2&#60;String, Integer> call(String x){
            return new Tuple2(x, 1);
          }}).reduceByKey(new Function2&#60;Integer, Integer, Integer>(){
              public Integer call(Integer x, Integer y){ return x+y;}});
  counts.saveAsTextFile("output/boa-constrictor");
  }
}
                        </code></pre>
                        <pre><code data-trim contenteditable>
>spark-submit --class WordCount --master local \
>spark-demo.jar
                        </code></pre>
                    </section>
				</section>
                
                <section>
                    <section>
                        <h2>Introduction to <a href="https://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a></h1>
                    </section>
                    <section data-background="img/digraph.png">
                    </section>
                    <section>
                        <h3>About GraphX</h3>
                        <ul>
                            <li>Only Scala API is Supported</li>
                            <li>Python Support Tracking in <a href="https://issues.apache.org/jira/browse/SPARK-3789">SPARK-3789</a>
                            <li><a href="https://github.com/graphframes/graphframes">Graph Frames</a> (GraphX on Dataframes) project will replace GraphX</a>
                            <li><a href="http://graphframes.github.io/">Graph Frames Documentation</a>
                        </ul>
                        <p>Significant overlap in API usage, <br>but Graph Frames is still in development. </p>
                    </section>
                    <section>
                        <h3>Key Terms</h3>
                        <div style ="height:100%">
                            <div style="height:50%">
        					<div style="width:100%;">
            					<div style="float:left; width:33%;">
                					<b><p>Vertex</p></b>
            					</div>
            					<div style="float:right; width:66%;">
                                    <div style="float:left; width:50%;">
                					<b><p>Edge</p></b>
                                    </div>
                                    <div style="float:right; width:50%;">
                					<b><p>Property Graph</p></b>
                                    </div>
            					</div>
                            </div>
                        </div>
                        <div style="height:50%">
                            <div style="width:100%">
                                <div style="width:25%;float:left"></div>
                                <div style="width:75%; float:right">
                                    <div style="width:66%; float:left">
                                        <pre><code  class="scala" data-trim contenteditable>
class Graph[VD, ED] {
  val vertices: VertexRDD[VD]
  val edges: EdgeRDD[ED]
}
                                        </code></pre></div>
                                    <div style="width:33%;float:right"></div>
                                </div>
                            </div>
                        </div>

                        </div>
                </section>

                    <section>
                        <h3>Relationship between Property Graph, Vertex Table, and Edge Table</h3>
    					<div style="width:100%;">
    					<img width="1600" src="img/property_graph.png", style="border:1px;"> 
    					</div>
                    </section>
                    
                </section>
                
                <section>
                    <section>
                        <h2>Appendix: How do I learn Spark?</h2>
                        <ul>
                            <li><a href="http://shop.oreilly.com/product/0636920028512.do">Learning Spark</a></li>
                            <li><a href="http://go.databricks.com/databricks-community-edition-beta-waitlist">Databricks Community Edition Beta Wishlist</a></li>
                            <li><a href="https://www.edx.org/course/big-data-analysis-spark-uc-berkeleyx-cs110x">Big Data Analysis with Spark (Included in Databricks)</a></li>
                            <li>Spark Summit Sessions and Tutorial Videos</li>
                    </section>
                    <section>
                        <h3>YouTube is Great (Find Newer Videos)</h3>
                        <iframe width="800" height="500" src="https://www.youtube.com/embed/VWeWViFCzzg?list=PLTPXxbhUt-YWSgAUhrnkyphnh0oKIT8-j" frameborder="0" allowfullscreen></iframe>
                    </section>
                    <section>
                        <iframe width="800" height="500" src="https://www.youtube.com/embed/HG2Yd-3r4-M" frameborder="0" allowfullscreen></iframe>
                    </section>
                </section>
                
                <section>
                    <section>
                        <h2>Appendix: Scala</h2>
                        <p style="font-size:50%">Content courtesy of David Der / <a href="https://twitter.com/davidder">@davidder</a></p>
                    </section>
                    <section>
                        <h2>Functional</h2>
                        <pre><code  class="java"  data-trim contenteditable>
public List&lt;Product&gt; getProducts(List&lt;Order&gt; orders) {

    List&lt;Product&gt; products = new ArrayList&lt;Product&gt;();

    for (Order order : orders) {
        products.addAll(order.getProducts());
        }

    return products;
}
                        </code></pre>
                        <h3>Vs.</h3>
                        <pre><code data-trim contenteditable>
&nbsp;
def products = orders.flatMap(o => o.products)
&nbsp;
                        </code></pre>
                    </section>
                    <section>
                        <h3>Spark operations look<br/>awfully familiar...</h3>
                        <ul>
                            <li>map( { .. } )</li>
                            <li>filter( { .. } )</li>
                            <li>flatMap( { .. } )</li>
                            <li>reduceByKey( { .. } )</li>
                            <li>reduce( { .. } )</li>
                            <li>foreach( { .. } )</li>
                        </ul>
                    </section>
                    
                    
                    <section>
                        <h2>Java 7</h2>
                        <pre><code data-trim contenteditable>
JavaRDD&lt;String&gt; distFile = sc.textFile("README.md");

// Map each line to multiple word
JavaRDD&lt;String&gt; words = distFile.flatMap(

    new FlatMapFunction&lt;String, String&gt;() {

        public Iterable&lt;String&gt; call(String line) {
            return Arrays.asList(line.split(" "));
        }
});
                        </code></pre>
                        <h2>Java 8</h2>
                        <pre><code data-trim contenteditable>
JavaRDD&lt;String&gt; distFile = sc.textFile("README.md");

JavaRDD&lt;String&gt; words =
distFile.flatMap(line -> Arrays.asList(line.split(" ")));
                        </code></pre>
                    </section>
                </section>

			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
